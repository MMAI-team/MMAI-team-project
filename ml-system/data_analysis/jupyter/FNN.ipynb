{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5879,"status":"ok","timestamp":1679938633997,"user":{"displayName":"Fraud Detection","userId":"06523487236688012022"},"user_tz":-180},"id":"jXMarHsfG3H8"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import datetime\n","from datetime import date, datetime\n","import tensorflow as tf\n","import json\n","import math\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","from sklearn import preprocessing\n","\n","class PreprocessorML():\n","    def __init__(self, normalization=False):\n","        self.norm = normalization\n","    \n","\n","    def one_hot_category(self, dataset):\n","        hot = ['gas_transport', 'grocery_pos', 'home', 'shopping_pos', 'kids_pets', 'shopping_net', 'entertainment',\n","               'food_dining', 'personal_care', 'health_fitness', 'misc_pos', 'misc_net', 'grocery_net', 'travel']\n","        \n","        for category in hot:\n","            dataset[category] = pd.Series([1 if x.category == category else 0 for x in dataset.itertuples()],\n","                                          index=dataset.index)\n","        \n","        return dataset\n","\n","\n","    def add_time(self, dataset):\n","        dataframe = dataset.sort_values(by=['cc_num', 'unix_time'])\n","\n","        delta_time = []\n","\n","        previous_row = dataframe.iloc[0]\n","\n","        delta_time.append(0)\n","\n","        for row in dataframe[1:].itertuples():\n","\n","            if row.cc_num == previous_row.cc_num:\n","                delta_time.append(row.unix_time - previous_row.unix_time)\n","            else:\n","                delta_time.append(0)\n","\n","            previous_row = row\n","\n","        dataframe['delta_time'] = pd.Series(delta_time, index=dataframe.index)\n","\n","        return dataframe\n","\n","\n","    def parse_time(self, string):\n","        return datetime.strptime(string, \"%Y-%m-%d %H:%M:%S\")\n","\n","\n","    def add_workhour_category(self, dataset):\n","        dataset['work_hours'] = dataset['trans_date_trans_time'].apply(\n","            lambda x: \n","                int(self.parse_time(x).hour >= 6 and self.parse_time(x).hour <= 18))\n","        return dataset\n","\n","\n","    def add_weekend_category(self, dataset):\n","        dataset['weekend'] = dataset['trans_date_trans_time'].apply(\n","            lambda x: \n","                int(self.parse_time(x).weekday() >= 5 and self.parse_time(x).weekday() <= 6))\n","        return dataset\n","\n","    \n","    def add_age(self, dataset):\n","        dataset['age'] = dataset['dob'].apply(lambda x: (date.today() - date.fromisoformat(x)).days // 365)\n","        return dataset\n","\n","\n","    def add_distance(self, dataset):\n","        lat1 = dataset['lat']\n","        lon1 = dataset['long']\n","        lat2 = dataset['merch_lat']\n","        lon2 = dataset['merch_long']\n","        dataset['distance'] = np.arccos(np.sin(lat1) * np.sin(lat2) + np.cos(lat1) * np.cos(lat2) * np.cos(lon1 - lon2)) * 6371\n","        return dataset\n","\n","    \n","    def add_gender(self, dataset):\n","        dataset['gender'] = pd.Categorical(dataset['gender'], categories=['F', 'M'])\n","        hot = pd.get_dummies(dataset['gender'], columns = ['F', 'M'])\n","        \n","        return dataset.join(hot)\n","\n","\n","    def add_weekday(self, dataset):\n","        dataset['weekday'] = dataset['trans_date_trans_time'].apply(\n","            lambda x: int(self.parse_time(x).weekday()))\n","        \n","        return dataset\n","    \n","    def add_hour(self, dataset):\n","        dataset['hour'] = dataset['trans_date_trans_time'].apply(\n","            lambda x: int(self.parse_time(x).hour))\n","        \n","        return dataset\n","\n","\n","    def preprocess(self, dataset, columns_to_delete):\n","        dataset = self.add_age(dataset)\n","        dataset = self.add_time(dataset)\n","        dataset = self.add_distance(dataset)\n","        dataset = self.one_hot_category(dataset)\n","        dataset = self.add_workhour_category(dataset)\n","        dataset = self.add_weekend_category(dataset)\n","        dataset = self.add_gender(dataset)\n","        print(type(dataset))\n","        dataset = self.add_hour(dataset)\n","        print(type(dataset))\n","        dataset = self.add_weekday(dataset)\n","        print(type(dataset))\n","\n","        dataset = dataset.drop(columns_to_delete, axis = 1)\n","\n","        return dataset\n","\n","class DataLoader():\n","    def __init__(self, batch_size=32, max_transactions=32):\n","        self.batch_size = batch_size\n","        self.max_transactions = max_transactions\n","\n","\n","    def add_padding_example(self, example):\n","        len(example[0])\n","        padding_ = [0 for x in example[0]]\n","\n","        while len(example) < self.max_transactions:\n","            example.append(padding_)\n","            \n","        return example\n","\n","\n","    def add_padding_label(self, label):\n","        padding_ = 2\n","\n","        while len(label) < self.max_transactions:\n","            label.append(padding_)\n","            \n","        return label\n","    \n","\n","    def divide_to_examples(self, dataset):\n","        examples = []\n","        labels = []\n","\n","        c_user = 0\n","        l_users = len(dataset.groupby('cc_num'))\n","\n","        for user_transactions in dataset.groupby('cc_num'):\n","\n","            user_df = user_transactions[1]\n","\n","            counter = 0\n","            while user_df.iloc[counter: counter + self.max_transactions].shape[0] != 0:\n","                current_example = user_df.iloc[counter: counter + self.max_transactions].copy().drop(['cc_num', 'is_fraud'], axis=1)\n","\n","                is_fraud = list(user_df.iloc[counter: counter + self.max_transactions]['is_fraud'])\n","\n","                array = current_example.to_numpy().tolist()\n","\n","                self.add_padding_example(array)\n","                self.add_padding_label(is_fraud)\n","\n","                labels.append(is_fraud)\n","                examples.append(array)\n","            \n","                counter += 1\n","            \n","            c_user += 1\n","\n","            if c_user % 1000 == 0:\n","                print('Dataset processed: ', c_user / l_users, '%')\n","\n","\n","        return np.asarray(examples), np.asarray(labels)\n","\n","\n","    def normalize_dataset(self, dataset, columns, norm='euclid', params_path=None, save=False):\n","        save_params = {}\n","        loaded_params = None\n","        if params_path!=None:\n","            f = open(params_path)\n","            loaded_params = json.load(f)\n","            f.close()\n","\n","        for column in columns:\n","            if norm == 'euclid':\n","                dataset[column] = preprocessing.normalize([dataset[column]])[0]\n","            elif norm == 'gauss':\n","                vector = tf.constant([list(dataset[column])], dtype=tf.float32)\n","                if loaded_params != None and column in loaded_params:\n","                    mean = loaded_params[column]['mean']\n","                    variance = loaded_params[column]['variance']\n","                else:\n","                    mean = tf.math.reduce_mean(vector, axis=1)\n","                    variance = tf.math.reduce_variance(vector, axis=1)[0]\n","\n","                dataset[column] = dataset[column].apply(lambda x: (x - float(mean)) / math.sqrt(float(variance)))\n","\n","                config_dict = {'mean': float(mean), 'variance': float(variance)}\n","                save_params[column] = config_dict\n","        if  save:       \n","            file = open('dataset_config.json', 'w')\n","            str_obj = json.dumps(save_params)\n","            file.write(str_obj)\n","            file.flush()\n","            file.close()\n","        return dataset\n","\n","    \n","    def process_batch(self, element_x, element_y):\n","        return {'example':element_x, 'label':element_y}\n","\n","\n","    def load_dataset(self, path, preprocess=False, columns_to_delete = [ \n","                      'city', \n","                      'dob', \n","                      'job', \n","                      'first', \n","                      'last',\n","                      'trans_date_trans_time',\n","                      'category',\n","                      'trans_num',\n","                      'lat',\n","                      'long',\n","                      'merch_lat',\n","                      'merch_long',\n","                      'unix_time',\n","                      'street',\n","                      'merchant',\n","                      'state',\n","                      'gender'], to_process='ALL', normalization=['delta_time', 'distance', 'city_pop'], example_weights=None,\n","                     divide=True, params_path=None):\n","        dataset = pd.read_csv(path, index_col=0)\n","        if to_process != 'ALL':\n","            dataset = dataset[:to_process]\n","\n","        # Preprocess dataset if preprocess == True\n","        if preprocess:\n","            preprocessor = PreprocessorML()\n","            dataset = preprocessor.preprocess(dataset, columns_to_delete)\n","        \n","        if normalization:\n","            dataset = self.normalize_dataset(dataset, normalization, norm='gauss', params_path=params_path)\n","\n","        dataset_X, dataset_y = None, None\n","        if divide:\n","            dataset_X, dataset_y = self.divide_to_examples(dataset)\n","        else:\n","            dataset_X, dataset_y = dataset.drop(['is_fraud', 'cc_num'], axis = 1).to_numpy().tolist(), list(dataset['is_fraud'])\n","        \n","\n","        dataset = tf.data.Dataset.from_tensor_slices((dataset_X, dataset_y))\n","\n","        return dataset.batch(self.batch_size).prefetch(AUTOTUNE)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":574},"executionInfo":{"elapsed":199562,"status":"error","timestamp":1679938833550,"user":{"displayName":"Fraud Detection","userId":"06523487236688012022"},"user_tz":-180},"id":"Y11UEot2HEhL","outputId":"a71c2bf5-a6f1-4d7c-ef34-d3f590f2d98f"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","<class 'pandas.core.frame.DataFrame'>\n","<class 'pandas.core.frame.DataFrame'>\n"]}],"source":["X = DataLoader(batch_size = 2048).load_dataset(path='../datasets/fraudTrain.csv',\n","                                 preprocess=True, divide=False, params_path='../../app/dataset_config.json', columns_to_delete = [ 'city', 'dob', 'job',  'first', 'last','trans_date_trans_time','category','trans_num',\n","                      'lat','long','merch_lat','merch_long','unix_time','street','merchant','state','zip','gender'], normalization=['amt', 'delta_time', 'distance', 'city_pop', 'age', 'hour', 'weekday'])"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1679938833551,"user":{"displayName":"Fraud Detection","userId":"06523487236688012022"},"user_tz":-180},"id":"eLZ4-eaEMTow"},"outputs":[],"source":["import tensorflow.keras as keras"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1679938833552,"user":{"displayName":"Fraud Detection","userId":"06523487236688012022"},"user_tz":-180},"id":"4BuZmKWkMRtY"},"outputs":[],"source":["\n","def make_model(output_bias=None):\n","  if output_bias is not None:\n","    output_bias = tf.keras.initializers.Constant(output_bias)\n","  input = keras.layers.Input(name='example', shape=(25,))\n","  x = keras.layers.Dense(256, activation='relu')(input)\n","  x = keras.layers.Dropout(0.1)(x)\n","  x = keras.layers.Dense(256, activation='relu')(x)\n","  x = keras.layers.Dropout(0.1)(x)\n","  x = keras.layers.Dense(128, activation='relu')(x)\n","  x = keras.layers.Dropout(0.1)(x)\n","  x = keras.layers.Dense(128, activation='relu')(x)\n","  x = keras.layers.Dropout(0.1)(x)\n","  x = keras.layers.Dense(64, activation='relu')(x)\n","  x = keras.layers.Dropout(0.1)(x)\n","  x = keras.layers.Dense(64, activation='relu')(x)\n","  x = keras.layers.Dropout(0.1)(x)\n","  x = keras.layers.Dense(32, activation='relu')(x)\n","  x = keras.layers.Dropout(0.1)(x)\n","  x = keras.layers.Dense(32, activation='relu')(x)\n","  x = keras.layers.Dropout(0.1)(x)\n","  output = keras.layers.Dense(1, activation='sigmoid')(x)\n","  model = tf.keras.models.Model(\n","        inputs=input, outputs=output, name=\"handwriting_recognizer\"\n","    )                                             \n","\n","  model.compile(\n","      loss=keras.losses.BinaryCrossentropy(),\n","      optimizer=keras.optimizers.Adam())\n","\n","  return model"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"U_xpPJWLJu9-"},"outputs":[],"source":["model = make_model()\n","model.load_weights('../../models/model_md_fnn_v3_0.h5')"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9433,"status":"ok","timestamp":1679838825484,"user":{"displayName":"Fraud Detection","userId":"06523487236688012022"},"user_tz":-180},"id":"MAToCCa5KaW9","outputId":"41193d6e-1f45-415a-8fc4-eaa79505ffbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["634/634 [==============================] - 5s 7ms/step\n"]}],"source":["preds = model.predict(X)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"zGZhmWqMMiNf"},"outputs":[],"source":["def convert_prob_to_class(y_probs, threshold=0.5):\n","    return (y_probs >= threshold).astype(int)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"R40Vz9tQM13D"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"5SC_cJKzM_xr"},"outputs":[],"source":["validation_d = []\n","validation_l = []\n","\n","for batch in X:\n","    validation_d.append(batch[0])\n","    validation_l.append(batch[1])\n","\n","val_dat = validation_d[0]\n","val_lab = validation_l[0]\n","for i in range(1, len(validation_d)):\n","    val_dat = np.concatenate((val_dat, validation_d[i]), axis=0)\n","    val_lab = np.concatenate((val_lab, validation_l[i]), axis=0)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80170,"status":"ok","timestamp":1679839049984,"user":{"displayName":"Fraud Detection","userId":"06523487236688012022"},"user_tz":-180},"id":"cqKKXGbRMonp","outputId":"881db7df-4fe9-4fca-c8e0-33121171485f"},"outputs":[{"name":"stdout","output_type":"stream","text":["40522/40522 [==============================] - 62s 2ms/step\n","Confusion Matrix:\n","[[1288640     529]\n"," [    281    7225]]\n"]}],"source":["y_pred = model.predict(val_dat)\n","# Converting the probabilities to binary predictions using a threshold of 0.5\n","y_pred = convert_prob_to_class(y_pred, threshold=0.5)\n","# Computing the confusion matrix\n","cm = confusion_matrix(val_lab, y_pred)\n","# Printing the confusion matrix\n","print(\"Confusion Matrix:\")\n","print(cm)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"tYihUExSNGgj"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\LiubomyrMaievskyi\\source\\repos\\other\\FraudDetectionSystem\\ml-system\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["model.save('../../models/model_md_fnn_new.h5')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOaH4WRqQUIrRhbuhuF8bI3","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
