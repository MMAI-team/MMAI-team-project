{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1>Is Same Whale Classifier</h1>\n","<p>This notebook will lead you through the process of training a model for classifying if two images are the images of the same whale or not.</p>\n","<p>Before you begin, you should have all the data needed already at hand and in the right format, as well as correct drivers and environment.</p>"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Set environment variable to increase stability on some systems (optionally)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""]},{"cell_type":"markdown","metadata":{},"source":["<h3>Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import json\n","import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import concurrent.futures\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.python.client import device_lib\n","from tensorflow.python.ops.math_ops import xdivy_eager_fallback\n","\n","from keras import datasets, layers, models, Model\n","from keras.models import load_model, Model\n","from keras.applications import resnet\n","from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n","from keras.optimizers import RMSprop\n","from keras import backend as K\n","from keras.utils import plot_model\n","import keras.utils as image\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from matplotlib import rcParams\n","\n","import datetime\n","from data_load.whale_data import load_data\n","\n","import coremltools"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Start Tensorboard session (optionally)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Set paths and constants</h3> \n","<p>Json-files should have three columns: img1, img2 and answer, where img1 and img2 contains names of images (including extensions), answer - whether those images are same (1) or different (0).</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7VwABM3iKE1"},"outputs":[],"source":["PATH_CSV = 'Data/train.csv'\n","PATH_PHOTOS = 'Data/train'\n","\n","TEST_JSON = 'Data/Test_Json_97_Full.json'\n","PATH_TEST_PHOTOS = 'Data/Test_Photos_97_Full'\n","\n","PATH_MODELS = 'Data/models/'\n","\n","TEST_SIZE = 0.1\n","VAL_SIZE = 0.2"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Read and split data</h3>\n","<p>Here, train data is split into train and validation datasets in a 3 to 1 ratio (originally this created a train/validation/test split of 0.6/0.2/0.2). Because we have test data saved completely separate from our train data &#150; no test dataset is taken from the train data.</p>\n","<p><font size='2'>Optionally: change '_' variable (wich represents test dataset taken from train data) to something meaningfull and use it in the code.</font></p>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = load_data(table_path=PATH_CSV, images_directory=PATH_PHOTOS)\n","\n","val_size_relative = VAL_SIZE/(1-TEST_SIZE)\n","\n","train_data, test_data = train_test_split(data, random_state=42,\n","                                         test_size=TEST_SIZE)\n","train_data, validate_data = train_test_split(train_data, random_state=42,\n","                                        test_size=val_size_relative)"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Define preprocessing functions</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PPDCnBt2iQii"},"outputs":[],"source":["def load_and_preprocess(path, scale=(112,112)):\n","  img= image.load_img(path, target_size=scale)\n","  img_array = image.img_to_array(img)\n","  preprocessed_img = resnet.preprocess_input(img_array)\n","  return preprocessed_img\n","\n","def load_and_preprocess_row(row, final_scale=(224,224)):\n","  img1_array = load_and_preprocess(row[0][0], final_scale)\n","  img2_array = load_and_preprocess(row[0][1], final_scale)\n","  return img1_array, img2_array"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Define data sequence class</h3>\n","<p>Sequences are generating data dynamically, wich allows us to work with giant datasets, while remaining multiprocess-safe.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class pair_sequence(keras.utils.Sequence):\n","  def __init__(self, data, batch_size=32, scale=(224,224,3),\n","                 n_classes=2, shuffle=True):\n","        self.indexes = None\n","        self.data = data\n","        self.batch_size = batch_size\n","        self.scale = scale\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","  def __len__(self):\n","        '''Denotes the number of batches per epoch'''\n","        return int(np.floor(len(self.indexes) / self.batch_size))\n","\n","  def __getitem__(self, index):\n","        '''Generate one batch of data'''\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","        batch_data = [self.data[k] for k in indexes]\n","        X, y = self.__data__generation(batch_data)\n","        return X, y\n","\n","  def on_epoch_end(self):\n","        '''Updates indexes after each epoch'''\n","        self.indexes = np.arange(len(self.data))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","  def __data__generation(self, batch_data):\n","    first = np.empty((self.batch_size, *self.scale))\n","    second = np.empty((self.batch_size, *self.scale))\n","\n","    y = np.empty((self.batch_size), dtype=float)\n","\n","    for count, row in enumerate(batch_data):\n","      first_img, second_img= load_and_preprocess_row(row,\n","                                          final_scale=self.scale)\n","      first[count] = first_img\n","      second[count] = second_img\n","\n","      y[count] = row[1]\n","    X = {'img_1': first, 'img_2': second}\n","    return X, y"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Use ResNet152 as a pretrained model</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_shape = (224, 224)\n","\n","base_cnn = resnet.ResNet152(\n","    weights=\"imagenet\", input_shape=target_shape + (3,), \n","    include_top=False, pooling='avg'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Use some intermediate ResNet layers as output in conjunction with regular output</h3>\n","<p>This allows our head model to look not only on high-level features, but on the low-level ones too.</p>\n","<p><font size='2'>Optionally: change layer_to_snap_names to experiment wich layers work best.</font></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14123,"status":"ok","timestamp":1697554378677,"user":{"displayName":"Oleksii Davydov","userId":"17021885182752025575"},"user_tz":-180},"id":"5r6B5PAzjeQm","outputId":"3f1e9087-0247-4ea3-92d8-1bf0bd453657"},"outputs":[],"source":["def snap_and_prepare_layer(cnn_to_snap_out_of, layer_name):\n","    snapped_layer = layers.GlobalAveragePooling2D()(cnn_to_snap_out_of.get_layer(layer_name).output)\n","    output = layers.Flatten()(snapped_layer)\n","    return output\n","\n","layer_to_snap_names = ['conv4_block36_out', 'conv4_block27_out', \n","                       'conv4_block18_out', 'conv4_block9_out', \n","                       'conv3_block8_out','conv3_block4_out', \n","                       'conv2_block3_out']\n","all_outputs=[]\n","\n","base_output = layers.Flatten()(base_cnn.output)\n","all_outputs.append(base_output)\n","\n","for name in layer_to_snap_names:\n","    all_outputs.append(snap_and_prepare_layer(base_cnn, name))\n","\n","embedding = Model(base_cnn.input, all_outputs, name=\"Embedding\")"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Freeze training of all layers in ResNet</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainable = False\n","for layer_nested in embedding.layers:\n","    layer_nested.trainable = trainable"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Define functions that help in creating head of the model</h3>\n","<p>Every pair of outputs (because seamese architecture will be used) of ResNet model, is going through some lambda layers, and every output of that goes through a small sequence of primitive layers, and all outputs are then concatenated.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGYlmah0kDxa"},"outputs":[],"source":["def simple_dense_subblock(starting_layer, units, dropout, block_number, subblock_name):\n","    starting_layer = layers.Dense(units, activation='relu', name=f'block{block_number}_sub{subblock_name}_dense1')(starting_layer)\n","    starting_layer = layers.BatchNormalization(name=f'block{block_number}_sub{subblock_name}_bn')(starting_layer)\n","    starting_layer = layers.Dropout(dropout, name=f'block{block_number}_sub{subblock_name}_dropout1')(starting_layer)\n","    starting_layer = layers.Dense(units, activation='relu', name=f'block{block_number}_sub{subblock_name}_dense2')(starting_layer)\n","    starting_layer = layers.Dropout(dropout, name=f'block{block_number}_sub{subblock_name}_dropout2')(starting_layer)\n","    return starting_layer\n","\n","def lambda_block(inputs, block_number, units):\n","    x1 = Lambda(lambda x: x[0] * x[1], name=f'block{block_number}_mult')(inputs)\n","    x1 = simple_dense_subblock(x1, units, 0.5, block_number, 'Mult')\n","    x2 = Lambda(lambda x: x[0] + x[1], name=f'block{block_number}_add')(inputs)\n","    x2 = simple_dense_subblock(x2, units, 0.5, block_number, 'Add')\n","    x3 = Lambda(lambda x: K.abs(x[0] - x[1]), name=f'block{block_number}_abs')(inputs)\n","    x4 = Lambda(lambda x: K.square(x), name=f'block{block_number}_square')(x3)\n","    x3 = simple_dense_subblock(x3, units, 0.5, block_number, 'Abs')\n","    x4 = simple_dense_subblock(x4, units, 0.5, block_number, 'Square')\n","    x5 = layers.Concatenate(name=f'block{block_number}_Concat')(inputs)\n","    x5 = simple_dense_subblock(x5, units, 0.3, block_number, 'Concat')\n","    return [x1, x2, x3, x4, x5]\n","\n","def head_block(input_1, input_2):\n","    head_ouput = []\n","    for idx in range(min(len(input_1), len(input_2))):\n","        units = int(list(input_1[idx].shape)[1]/8)\n","        head_ouput.extend(lambda_block([input_1[idx], input_2[idx]], idx, units))\n","    output_layer = layers.Concatenate()(head_ouput)\n","    return output_layer"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Create model with seamese architecture</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img_1_input = layers.Input(name=\"img_1\", shape=target_shape + (3,))\n","img_2_input = layers.Input(name=\"img_2\", shape=target_shape + (3,))\n","\n","all_outputs_1 = embedding(img_1_input)\n","all_outputs_2 = embedding(img_2_input)\n","\n","output = head_block(all_outputs_1, all_outputs_2)\n","\n","output = layers.Dropout(0.2)(output)\n","output = layers.Dense(1)(output)\n","\n","model = Model([img_1_input, img_2_input], output)"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Plot the model (optionally)</h3>\n","<p>Installing graphviz may be required. If model is not shown, see instructions <a href=\"https://graphviz.gitlab.io/download/\">here</a>, and restart the kernel</p>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf.keras.utils.plot_model(model, to_file='siamese_model.png', show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Initialize data sequences</h3>\n","<p>Change in batch_size may be needed in some systems, depending on RAM<p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orgjhUO0m7kP"},"outputs":[],"source":["train_sequence = pair_sequence(train_data, batch_size=16, shuffle=True, scale=(224, 224, 3))\n","validate_sequence = pair_sequence(validate_data, batch_size=16, shuffle=True, scale=(224, 224, 3))\n","test_sequence = pair_sequence(test_data, batch_size=16, shuffle=False, scale=(224, 224, 3))"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Initialize optimiser</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rms = keras.optimizers.RMSprop(learning_rate=1e-4)"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Compile the model</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.compile(loss=keras.losses.BinaryCrossentropy(),\n","              optimizer=rms, metrics=['binary_accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Initialize callback functions</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["checkpoint_filepath = 'Data/models/tmp/checkpoint'\n","log_dir='logs/fit/'+datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_binary_accuracy',\n","    mode='max',\n","    save_best_only=True)\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Train the model</h3>\n","<p>On some low-end systems, using tensorboard_callback may crash the kernel. Delete it from the list of callbacks, if this happens.</p> "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = model.fit(\n","    train_sequence,\n","    initial_epoch=0,\n","    epochs=50,\n","    validation_data = validate_sequence,\n","    steps_per_epoch=len(train_sequence),\n","    use_multiprocessing=True,\n","    validation_steps=len(validate_sequence),\n","    verbose=1,\n","    workers=4,\n","    max_queue_size=10,\n","    callbacks=[model_checkpoint_callback, tensorboard_callback])"]},{"cell_type":"markdown","metadata":{},"source":["<h3>If Tensorboard was used, view the training process (optionally)</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%tensorboard --logdir logs/fit"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Save or Load models (optionally)</h3>\n","<p>Input from the user is needed to prevent accidental overwriting.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if input('Are you sure you want to save the model?') == 'yes':\n","    model.save(PATH_MODELS+'CNN.h5', save_format='h5')\n","else:\n","    print('Not Saved')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if input('Are you sure you want to load the model?') == 'yes':\n","    model = keras.models.load_model(PATH_MODELS+'model_name.h5')\n","else:\n","    print('Not Loaded')"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Convert to .coreml (optionally)</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coreml_model = coremltools.convert(model, convert_to='mlprogram')\n","coreml_model.save('CNN.mlpackage')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if input('Are you sure you want to load and save the checkpoint model?') == 'yes':\n","    model.load_weights(checkpoint_filepath)\n","    model.save(PATH_MODELS+'model_name_checkpoint.h5', save_format='h5')\n","else:\n","    print('Not Saved')"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Change learning rate of a model (optionally)</h3>\n","<p>This step usually occurs after loading a model, before the next training iteration</p>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["K.set_value(model.optimizer.learning_rate, 1e-5)"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Unfreeze training of ResNet layers (optionally)</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainable=True\n","for layer_nested in model.get_layer('Embedding').layers:\n","    layer_nested.trainable = trainable\n"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Check if training states of the layers are correct (optionally)</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.summary(expand_nested=True, show_trainable=True)"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Re-check how model performs on validation data (optionally)</h3>\n","<p>This is usually done to check if correct model is loaded.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validate_sequence = pair_sequence(validate_data, batch_size=16, shuffle=False, scale=(224, 224, 3))\n","\n","val_prediction = model.predict(\n","    validate_sequence,\n","    verbose=1,\n","    steps = len(validate_sequence)+1,\n","    use_multiprocessing=True\n",")\n","result = list(map(lambda x: 0 if x<0.5 else 1, val_prediction))\n","trues=[x[1] for x in validate_data]\n","print(classification_report(trues, result[:len(trues)], digits=4))"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Test the model</h3>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_prediction = model.predict(\n","    test_sequence,\n","    verbose=1,\n","    steps = len(test_sequence)+1,\n","    use_multiprocessing=True\n",")\n","result = list(map(lambda x: 0 if x<0.5 else 1, test_prediction))\n","trues=[x[1] for x in test_data]\n","print(classification_report(trues, result[:len(trues)], digits=2))"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyP/Azu2tAFbkxo7ANMNinSq","mount_file_id":"1MEXiGBoHaAD6xq8Y2vBWoJam1EzpA9ci","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
